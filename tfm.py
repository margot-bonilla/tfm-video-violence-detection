# -*- coding: utf-8 -*-
"""TFM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rSovsbHkt-usYieS5x_c5fI7SnT3nYAP

# Steps to do for this escenario
* Video Preprocessing:

  * **Frame Extraction:** You'll need to extract individual frames from each video. Depending on your problem, you might extract frames at regular intervals or use all frames in the video.
  * **Data Augmentation:** Since your dataset is relatively small, data augmentation becomes crucial. Apply techniques like random cropping, flipping, rotation, and brightness adjustments to artificially increase your effective dataset size.
* Temporal Models:

    **3D Convolutional Neural Networks (3D CNNs):** These are extensions of 2D CNNs to video data. They can learn spatiotemporal features directly from video sequences. Popular architectures include C3D and I3D.
* Time-Series Approaches:

    **LSTM (Long Short-Term Memory) Networks:** LSTM networks are recurrent neural networks designed to handle sequential data. You can treat each video as a sequence of frames and use LSTMs to capture temporal dependencies.
* Action Recognition:

    **Two-Stream CNNs:** These models use separate streams for spatial and temporal information. The spatial stream processes individual frames, while the temporal stream captures motion information. They can be combined at the end to make predictions.
* Transfer Learning and Pretrained Models:

    **Pretrained 3D CNNs:** Models pretrained on large video datasets (like Kinetics or UCF101) can provide strong features that you can fine-tune for your specific task.
* Data Efficiency:

    **Self-Supervised Learning:** Given your limited dataset, consider exploring self-supervised learning approaches where you design tasks that allow the model to learn from the data itself, reducing the reliance on labeled examples.
* Data Subset and Strategies:

    Due to the relatively small dataset size, consider using techniques like k-fold cross-validation to ensure robust evaluation. You might also experiment with stratified sampling to ensure a balanced distribution of classes across training and validation sets.
* Model Complexity:

    Be mindful of the complexity of the models you choose. Extremely deep or complex models might struggle to generalize well given the limited dataset size. Regularization techniques can help mitigate overfitting.

# Frame Extraction

## Function Definition
"""

# Constants
num_frames_per_video = 50
frame_width = 224
frame_height = 224

import cv2
import os
import numpy as np


def extract_frames(video_path, num_frames, output_path):
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    segment_size = total_frames // num_frames

    frames = []
    for i in range(num_frames):
        frame_idx = i * segment_size
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        if ret:
            frame = cv2.resize(frame, (frame_width, frame_height))
            frames.append(frame)

    cap.release()

    for idx, frame in enumerate(frames):
        frame_filename = f'frame_{idx:04d}.jpg'
        frame_path = os.path.join(output_path, frame_filename)
        cv2.imwrite(frame_path, frame)


def process_video(video_file, video_folder, output_folder):
    video_path = os.path.join(video_folder, video_file)
    video_name = os.path.splitext(video_file)[0]

    output_path = os.path.join(output_folder, video_name)
    os.makedirs(output_path, exist_ok=True)

    extract_frames(video_path, num_frames_per_video, output_path)
    print(f"Processed video: {video_name}")

from functools import partial
import multiprocessing


def multiprocess_multiple_frame_extraction(video_folder, output_folder):
  os.makedirs(output_dir, exist_ok=True)

  # Get list of video files
  video_files = os.listdir(video_folder)

  # Create a partial function with fixed arguments
  partial_process_video = partial(process_video, video_folder=video_folder, output_folder=output_folder)

  # Use multiprocessing to parallelize the process
  num_processes = multiprocessing.cpu_count()  # Use available CPU cores
  with multiprocessing.Pool(processes=num_processes) as pool:
      pool.map(partial_process_video, video_files)

  print(f'All videos for {video_folder} processed and set in {output_folder}.')

"""## Function Execution"""

# set all frames for violent Movies

video_folder = '/content/drive/MyDrive/TFM/Movies/fights'
output_folder = '/content/drive/MyDrive/TFM/NotebookWork/frames_2/violent/'
multiprocess_multiple_frame_extraction(video_folder, output_folder)

# Set all frames for non violent Movies

video_dir = '/content/drive/MyDrive/TFM/Movies/noFights'
output_dir = '/content/drive/MyDrive/TFM/NotebookWork/frames_2/non_violent/'
multiprocess_multiple_frame_extraction(video_dir, output_dir)

# Python program to rename all file
# names in your directory
import os

os.chdir('/content/drive/MyDrive/TFM/violence-detection-dataset/non-violent/cam1')
print(os.getcwd())

for count, f in enumerate(os.listdir()):
	f_name, f_ext = os.path.splitext(f)
	f_name = f"cam_1_{f_name}"

	new_name = f'{f_name}{f_ext}'
	os.rename(f, new_name)

# Set all frames for violent CAM1

video_dir = '/content/drive/MyDrive/TFM/violence-detection-dataset/violent/cam1'
output_dir = '/content/drive/MyDrive/TFM/NotebookWork/frames_2/violent/'
multiprocess_multiple_frame_extraction(video_dir, output_dir)

# Set all frames for violent CAM2

video_dir = '/content/drive/MyDrive/TFM/violence-detection-dataset/violent/cam2'
output_dir = '/content/drive/MyDrive/TFM/NotebookWork/frames_2/violent/'
multiprocess_multiple_frame_extraction(video_dir, output_dir)

# Set all frames for non violent CAM1

video_dir = '/content/drive/MyDrive/TFM/violence-detection-dataset/non-violent/cam1'
output_dir = '/content/drive/MyDrive/TFM/NotebookWork/frames_2/non_violent/'
multiprocess_multiple_frame_extraction(video_dir, output_dir)

# Set all frames for non violent CAM2

video_dir = '/content/drive/MyDrive/TFM/violence-detection-dataset/non-violent/cam2'
output_dir = '/content/drive/MyDrive/TFM/NotebookWork/frames_2/non_violent/'
multiprocess_multiple_frame_extraction(video_dir, output_dir)

# Set all frames for non violent Hockey

video_dir = '/content/drive/MyDrive/TFM/HockeyFights/no-violent'
output_dir = '/content/drive/MyDrive/TFM/NotebookWork/frames_2/non_violent/'
multiprocess_multiple_frame_extraction(video_dir, output_dir)

# Set all frames for violent Hockey

video_dir = '/content/drive/MyDrive/TFM/HockeyFights/violent'
output_dir = '/content/drive/MyDrive/TFM/NotebookWork/frames_2/violent/'
multiprocess_multiple_frame_extraction(video_dir, output_dir)

"""# Feature Extraction"""

import os
import cv2
import numpy as np
import torch
import torchvision.transforms as transforms
import torchvision.models as models

from PIL import Image


# Set up paths
data_dir = '/content/drive/MyDrive/TFM/NotebookWork/frames_2'
violent_dir = os.path.join(data_dir, 'violent')
non_violent_dir = os.path.join(data_dir, 'non_violent')

# Preprocessing for the pre-trained CNN
preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load a pre-trained CNN model
cnn_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
cnn_model.eval()  # Set the model to evaluation mode

def extract_features_from_frames(video_folder):
    features = []
    for video in os.listdir(video_folder):
        video_path = os.path.join(video_folder, video)

        video_features = []  # To store features for frames in the video
        for frame_file in os.listdir(video_path):
            frame_path = os.path.join(video_path, frame_file)
            frame = cv2.imread(frame_path)

            # Extract features using OpenCV (Histogram of Oriented Gradients)
            hog = cv2.HOGDescriptor()
            hog_features = hog.compute(frame)

            # Convert NumPy array to PIL image
            frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

            # Extract features using pre-trained CNN
            frame_tensor = preprocess(frame_pil)
            frame_tensor = torch.unsqueeze(frame_tensor, 0)
            with torch.no_grad():
                cnn_features = cnn_model(frame_tensor)

            video_features.append(np.concatenate((hog_features.flatten(), cnn_features.numpy().flatten())))

        # Aggregate features for the entire video
        aggregated_features = np.mean(video_features, axis=0)

        features.append(aggregated_features)

    return features

# Extract features from violent and non-violent frames
violent_features = extract_features_from_frames(violent_dir)
non_violent_features = extract_features_from_frames(non_violent_dir)

# Create labels (1 for violent, 0 for non-violent)
labels = np.concatenate((np.ones(len(violent_features)), np.zeros(len(non_violent_features))))

# Combine features and labels
all_features = np.concatenate((violent_features, non_violent_features), axis=0)

# Multiprocessing to increase velocity

import os
import cv2
import numpy as np
import torch
import torchvision.transforms as transforms
import torchvision.models as models
from PIL import Image
from multiprocessing import Pool


# Set up paths
data_dir = '/content/drive/MyDrive/TFM/NotebookWork/frames_2'
violent_dir = os.path.join(data_dir, 'violent')
non_violent_dir = os.path.join(data_dir, 'non_violent')

# Preprocessing for the pre-trained CNN
preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load a pre-trained CNN model
cnn_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
cnn_model.eval()


def extract_features_from_frame(frame_path):
    frame = cv2.imread(frame_path)

    hog = cv2.HOGDescriptor()
    hog_features = hog.compute(frame)

    frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    frame_tensor = preprocess(frame_pil)
    frame_tensor = torch.unsqueeze(frame_tensor, 0)
    with torch.no_grad():
        cnn_features = cnn_model(frame_tensor)

    return np.concatenate((hog_features.flatten(), cnn_features.numpy().flatten()))

def extract_features_from_video(video_folder):
    video_features = []
    for frame_file in os.listdir(video_folder):
        frame_path = os.path.join(video_folder, frame_file)
        video_features.append(extract_features_from_frame(frame_path))
    aggregated_features = np.mean(video_features, axis=0)
    return aggregated_features

def extract_features_from_frames(video_folder):
    frame_paths = [os.path.join(video_folder, frame_file) for frame_file in os.listdir(video_folder)]
    with Pool(processes=os.cpu_count()) as pool:
        features = pool.map(extract_features_from_frame, frame_paths)
    return features


# Extract features from violent and non-violent frames using multiprocessing
violent_features = extract_features_from_frames(violent_dir)
non_violent_features = extract_features_from_frames(non_violent_dir)

# Create labels (1 for violent, 0 for non-violent)
labels = np.concatenate((np.ones(len(violent_features)), np.zeros(len(non_violent_features))))

# Combine features and labels
all_features = np.concatenate((violent_features, non_violent_features), axis=0)

"""# Models Definition

## Convolutional Neural Networks (CNNs)
CNNs are widely used for image and video analysis tasks due to their ability to capture spatial hierarchies of features
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

model_cnn = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')  # Binary classification output
])

"""## Recurrent Neural Networks (RNNs):
To capture temporal information across frames, RNNs or Long Short-Term Memory (LSTM) networks are suitable choices. It is possible to treat each video as a sequence of frames and use RNNs to process them.
"""

feature_size = 128

from tensorflow.keras.layers import LSTM

model_rnn = Sequential([
    LSTM(128, input_shape=(num_frames_per_video, feature_size), return_sequences=True),
    LSTM(64),
    Dense(1, activation='sigmoid')  # Binary classification output
])

"""##3D Convolutional Neural Networks (3D CNNs):
To capture both spatial and temporal features directly from video frames, 3D CNNs are a suitable choice. These networks operate on 3D tensors where one dimension represents time.

"""

model_3dcnn = Sequential([
    tf.keras.layers.Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(num_frames_per_video, 224, 224, 3)),
    tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2)),
    tf.keras.layers.Conv3D(64, kernel_size=(3, 3, 3), activation='relu'),
    tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2)),
    tf.keras.layers.Conv3D(128, kernel_size=(3, 3, 3), activation='relu'),
    tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification output
])

"""# Model Training"""

epochs = 70
patience = 15

from tensorflow.keras.callbacks import EarlyStopping

# Define an early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)

import os
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split

# Split the dataset into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(all_features, labels, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Reshape the input data if necessary (depends on the model architecture)
X_train = X_train.reshape((-1, num_frames_per_video, feature_size))
X_val = X_val.reshape((-1, num_frames_per_video, feature_size))
X_test = X_test.reshape((-1, num_frames_per_video, feature_size))

# Model 1: Convolutional Neural Network (CNN)
model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_cnn.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, callbacks=[early_stopping])
model_cnn.save('model_cnn.h5')

# Model 2: Recurrent Neural Network (RNN)
model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_rnn.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, callbacks=[early_stopping])
model_rnn.save('model_rnn.h5')

# Model 3: 3D Convolutional Neural Network (3D CNN)
model_3dcnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_3dcnn.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, callbacks=[early_stopping])
model_3dcnn.save('model_3dcnn.h5')

